---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I've just graduated from Carnegie Mellon University's School of Computer Science. I’m passionate about investigating novel solutions for solving computational challenges. I’m experienced in developing efficient algorithms and system architectures to handle large-scale computational-intensive tasks. During my two years of experience as a research assistant at CMU, I contributed to cutting-edge interdisciplinary research in areas like bioinformatic analysis and multimodal sensory information processing. I published in prestigious journals, including Nature Biotechnology, Bioinformatics and Transactions in Machine Learning Research,  showcasing my commitment to rigorous research and impactful results. I also worked as a teaching assistant for both undergraduate and graduate Machine Learning and System courses at CMU. 

While conducting my studies and research, I’ve witnessed a rapid growth in the parameter size and structural complexity of state-of-the-art deep learning models. Meanwhile, the high memory utilization of large models led to a skyrocketing cost for AI development on cloud computing platforms. Having worked as an individual researcher and technical lead for a startup, I have first hand experience on the importance and huge economic benefit for democratizing AI deployment on heterogenous, decentralized, or even edge hardware platforms. I’m highly motivated to tackle the following two problems in the area, 1. How to generate placement policy to place large computational graphs on scattered heterogeneous resources; 2. How to achieve inference or fine-tuning of large models on lightweight edge devices with limited GPU memories. 

With these questions in mind, I’m currently exploring large language model fine-tuning on commodity hardware platforms using offloading techniques with Professor Phillip Gibbons. I adopted a runtime memory manager that generates placement policies for each operation (on CPU or GPU) based on future tensor access patterns. I used this technique along with stochastic depth in neural network training to enable democratization of large model finetuning on commodity hardware platforms.  I also plan to scale our implementation to a distributed or edge setting.

I am enthusiastic about connecting with like-minded professionals, exploring opportunities for collaboration, and making a positive impact in the world of technology.

Education Background
----
2018 - 2022:
Bachelar of Science in Computer Science
Minor in Mathematical Science

2022 - 2023:
Master of Science in Computer Science (Thesis Based)

Expertise
----
Algorithm Development

Machine Learning

Natural Language Processing

Distributed Systems

Software Engineering

Research Interest 
----
Distributed Machine Learning

Bioinformatics and Biostatistics

Multimodal Machine Learning

Theoretical Machine Learning and Optimizations

Currently Looking for 
----
Ph.D. in Computer Science, Bioinformatics or Electrical Engineering

Software/ML Engineering position in Industry
